{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Version</th>\n",
       "      <th>DataFrom</th>\n",
       "      <th>DataMethod</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>RaceWht</th>\n",
       "      <th>RaceAsn</th>\n",
       "      <th>RaceInd</th>\n",
       "      <th>...</th>\n",
       "      <th>cntLOCSameIndex</th>\n",
       "      <th>cntLOCBefore15yr</th>\n",
       "      <th>YoungestAgeLOC</th>\n",
       "      <th>cntModSevInjuries</th>\n",
       "      <th>cntModSevBeforeIndex</th>\n",
       "      <th>cntModSevAfterIndex</th>\n",
       "      <th>cntModSevSameIndex</th>\n",
       "      <th>cntModSevBefore15yr</th>\n",
       "      <th>RURALadm</th>\n",
       "      <th>RURALdc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16134.000000</td>\n",
       "      <td>16134.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>757.000000</td>\n",
       "      <td>16128.000000</td>\n",
       "      <td>16126.000000</td>\n",
       "      <td>11419.000000</td>\n",
       "      <td>11411.000000</td>\n",
       "      <td>11350.000000</td>\n",
       "      <td>11347.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12413.000000</td>\n",
       "      <td>12413.000000</td>\n",
       "      <td>1754.000000</td>\n",
       "      <td>12414.000000</td>\n",
       "      <td>12413.000000</td>\n",
       "      <td>12413.000000</td>\n",
       "      <td>12413.000000</td>\n",
       "      <td>12413.000000</td>\n",
       "      <td>13788.000000</td>\n",
       "      <td>13742.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8067.500000</td>\n",
       "      <td>10.192829</td>\n",
       "      <td>5.825627</td>\n",
       "      <td>6.143989</td>\n",
       "      <td>1.735739</td>\n",
       "      <td>1.752201</td>\n",
       "      <td>1.232245</td>\n",
       "      <td>1.838577</td>\n",
       "      <td>1.153568</td>\n",
       "      <td>1.159690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007250</td>\n",
       "      <td>0.048900</td>\n",
       "      <td>25.046750</td>\n",
       "      <td>0.045271</td>\n",
       "      <td>0.031016</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.008781</td>\n",
       "      <td>2.013780</td>\n",
       "      <td>2.039951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4657.628957</td>\n",
       "      <td>5.069536</td>\n",
       "      <td>20.789223</td>\n",
       "      <td>19.918462</td>\n",
       "      <td>0.444873</td>\n",
       "      <td>1.383127</td>\n",
       "      <td>0.909270</td>\n",
       "      <td>0.921011</td>\n",
       "      <td>0.934800</td>\n",
       "      <td>0.945018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089466</td>\n",
       "      <td>0.258487</td>\n",
       "      <td>17.030327</td>\n",
       "      <td>0.249798</td>\n",
       "      <td>0.207237</td>\n",
       "      <td>0.115777</td>\n",
       "      <td>0.035881</td>\n",
       "      <td>0.099969</td>\n",
       "      <td>0.749332</td>\n",
       "      <td>0.747169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4034.250000</td>\n",
       "      <td>9.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8067.500000</td>\n",
       "      <td>11.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12100.750000</td>\n",
       "      <td>14.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16134.000000</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0       Version    DataFrom  DataMethod           Sex  \\\n",
       "count  16134.000000  16134.000000  757.000000  757.000000  16128.000000   \n",
       "mean    8067.500000     10.192829    5.825627    6.143989      1.735739   \n",
       "std     4657.628957      5.069536   20.789223   19.918462      0.444873   \n",
       "min        1.000000      0.000000    0.000000    1.000000      1.000000   \n",
       "25%     4034.250000      9.100000    0.000000    1.000000      1.000000   \n",
       "50%     8067.500000     11.300000    0.000000    1.000000      2.000000   \n",
       "75%    12100.750000     14.100000    1.000000    1.000000      2.000000   \n",
       "max    16134.000000     16.100000   99.000000   99.000000      9.000000   \n",
       "\n",
       "               Race     Ethnicity       RaceWht       RaceAsn       RaceInd  \\\n",
       "count  16126.000000  11419.000000  11411.000000  11350.000000  11347.000000   \n",
       "mean       1.752201      1.232245      1.838577      1.153568      1.159690   \n",
       "std        1.383127      0.909270      0.921011      0.934800      0.945018   \n",
       "min        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "25%        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "50%        1.000000      1.000000      2.000000      1.000000      1.000000   \n",
       "75%        2.000000      1.000000      2.000000      1.000000      1.000000   \n",
       "max        9.000000      9.000000      9.000000      9.000000      9.000000   \n",
       "\n",
       "       ...  cntLOCSameIndex  cntLOCBefore15yr  YoungestAgeLOC  \\\n",
       "count  ...     12413.000000      12413.000000     1754.000000   \n",
       "mean   ...         0.007250          0.048900       25.046750   \n",
       "std    ...         0.089466          0.258487       17.030327   \n",
       "min    ...         0.000000          0.000000        0.000000   \n",
       "25%    ...         0.000000          0.000000       13.000000   \n",
       "50%    ...         0.000000          0.000000       20.000000   \n",
       "75%    ...         0.000000          0.000000       32.750000   \n",
       "max    ...         2.000000          6.000000       95.000000   \n",
       "\n",
       "       cntModSevInjuries  cntModSevBeforeIndex  cntModSevAfterIndex  \\\n",
       "count       12414.000000          12413.000000         12413.000000   \n",
       "mean            0.045271              0.031016             0.011440   \n",
       "std             0.249798              0.207237             0.115777   \n",
       "min             0.000000              0.000000             0.000000   \n",
       "25%             0.000000              0.000000             0.000000   \n",
       "50%             0.000000              0.000000             0.000000   \n",
       "75%             0.000000              0.000000             0.000000   \n",
       "max             7.000000              6.000000             3.000000   \n",
       "\n",
       "       cntModSevSameIndex  cntModSevBefore15yr      RURALadm       RURALdc  \n",
       "count        12413.000000         12413.000000  13788.000000  13742.000000  \n",
       "mean             0.001289             0.008781      2.013780      2.039951  \n",
       "std              0.035881             0.099969      0.749332      0.747169  \n",
       "min              0.000000             0.000000      1.000000      1.000000  \n",
       "25%              0.000000             0.000000      1.000000      1.000000  \n",
       "50%              0.000000             0.000000      2.000000      2.000000  \n",
       "75%              0.000000             0.000000      3.000000      3.000000  \n",
       "max              1.000000             3.000000      3.000000      3.000000  \n",
       "\n",
       "[8 rows x 264 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "form1_file_path = './tbimsform1.csv'\n",
    "form1_data = pd.read_csv(form1_file_path)\n",
    "form1_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'UID', 'Version', 'DataFrom', 'DataMethod', 'Sex', 'Race',\n",
       "       'Ethnicity', 'RaceWht', 'RaceAsn',\n",
       "       ...\n",
       "       'cntLOCSameIndex', 'cntLOCBefore15yr', 'YoungestAgeLOC',\n",
       "       'cntModSevInjuries', 'cntModSevBeforeIndex', 'cntModSevAfterIndex',\n",
       "       'cntModSevSameIndex', 'cntModSevBefore15yr', 'RURALadm', 'RURALdc'],\n",
       "      dtype='object', length=266)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "form1_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_data = form1_data[pd.notnull(form1_data['CTFrag'])]\n",
    "form_data = form_data[pd.notnull(form_data['CTPunctate'])]\n",
    "form_data = form_data[pd.notnull(form_data['SCI'])]\n",
    "final = form_data[pd.notnull(form_data['CT7c1AxialLNS'])]\n",
    "final = final[pd.notnull(final['CT5b3CorticalNTemp'])]\n",
    "final = final[pd.notnull(final['CT7c1AxialLNS'])]\n",
    "final = final[pd.notnull(final['CT5b1CorticalLTemp'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['DAYStoREHABadm', 'CTIntracrain', 'CTPunctate', 'CTSubarachnioid', 'CTIntraventricular', 'CT5a1CorticalLFront', 'CT5a2CorticalRFront', 'CT5a3CorticalNFront', 'CT5b1CorticalLTemp', 'CT5b2CorticalRTemp', 'CT5b3CorticalNTemp', 'CT5c1CorticalLPar', 'CT5c2CorticalRPar', 'CT5c3CorticalNPar', 'CT5d1CorticalLOcc', 'CT5d2CorticalROcc', 'CT5d3CorticalNOcc', 'CT5e1CorticalLUnk', 'CT5e2CorticalRUnk', 'CT5e3CorticalNUnk', 'CT6aNonCortL', 'CT6aNonCortR', 'CT6aNonCortN', 'CT7a1AxialLEpi', 'CT7a2AxialREpi', 'CT7a3AxialNEpi', 'CT7b1AxialLSub', 'CT7b2AxialRSub', 'CT7b3AxialNSub', 'CT7c1AxialLNS', 'CT7c2AxialRNS', 'CT7c3AxialNNS', 'CT7d1FalcineSub', 'CT7d2FalcineSAH', 'CT7d3FalcineUnk', 'CTFrag', 'SCI']\n",
    "# add more continuous features? Add CT score thing as a Binary variable?\n",
    "for variable in features:\n",
    "    final = final[pd.notnull(final[variable])]\n",
    "X = final[features]\n",
    "y = X.DAYStoREHABadm # we now have consistent filtering!\n",
    "needLater = X.DAYStoREHABadm\n",
    "X = X.drop(columns=\"DAYStoREHABadm\")\n",
    "# # m = pd.get_dummies(X.Drugs)\n",
    "# # m.describe()\n",
    "# # X.describe()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_test.shape, y_test.shape)\n",
    "\n",
    "# # to test other models, uncomment the line\n",
    "# days_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=10000)\n",
    "# y_pred = days_model.fit(X_train, y_train).predict(X_test)\n",
    "# print(\"Number of mislabeled points out of a total of %d points: %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "# accuracy_score(y_test,y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we want to find the one CT variable that is the biggest predictor - so two approaches:\n",
    "1. look at variable that the lack of results in a large loss of accuracy\n",
    "2. look at variable which individually has considerable predictive power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:08<00:00,  1.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTIntraventricular 0.7323943661971831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm \n",
    "\n",
    "#approach 1:\n",
    "features = ['DAYStoREHABadm', 'CTIntracrain', 'CTPunctate', 'CTSubarachnioid', 'CTIntraventricular', 'CT5a1CorticalLFront', 'CT5a2CorticalRFront', 'CT5a3CorticalNFront', 'CT5b1CorticalLTemp', 'CT5b2CorticalRTemp', 'CT5b3CorticalNTemp', 'CT5c1CorticalLPar', 'CT5c2CorticalRPar', 'CT5c3CorticalNPar', 'CT5d1CorticalLOcc', 'CT5d2CorticalROcc', 'CT5d3CorticalNOcc', 'CT5e1CorticalLUnk', 'CT5e2CorticalRUnk', 'CT5e3CorticalNUnk', 'CT6aNonCortL', 'CT6aNonCortR', 'CT6aNonCortN', 'CT7a1AxialLEpi', 'CT7a2AxialREpi', 'CT7a3AxialNEpi', 'CT7b1AxialLSub', 'CT7b2AxialRSub', 'CT7b3AxialNSub', 'CT7c1AxialLNS', 'CT7c2AxialRNS', 'CT7c3AxialNNS', 'CT7d1FalcineSub', 'CT7d2FalcineSAH', 'CT7d3FalcineUnk', 'CTFrag', 'SCI']\n",
    "# add more continuous features? Add CT score thing as a Binary variable?\n",
    "for variable in features:\n",
    "    final = final[pd.notnull(final[variable])]\n",
    "X = final[features]\n",
    "y = X.DAYStoREHABadm # we now have consistent filtering!\n",
    "needLater = X.DAYStoREHABadm\n",
    "X = X.drop(columns=\"DAYStoREHABadm\")\n",
    "# m = pd.get_dummies(X.Drugs)\n",
    "# m.describe()\n",
    "# X.describe()\n",
    "original = X\n",
    "minAccuracyScore = 2334\n",
    "maxEffect = 0\n",
    "print(len(features))\n",
    "y = needLater\n",
    "y = pd.cut(y, [-1, 27, 1000], labels=False) + 1\n",
    "maxEffects = [] # the most frequent element in this is likely the biggest factor?\n",
    "for i in tqdm(range(100)):\n",
    "    for feature in features[1:]:\n",
    "        X = X.drop(columns=feature)\n",
    "    #     X = X.reshape(-1,1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "#         print(X_train.shape, y_train.shape)\n",
    "#         print(X_test.shape, y_test.shape)\n",
    "\n",
    "        # to test other models, uncomment the line\n",
    "        days_model = GaussianNB()\n",
    "        y_pred = days_model.fit(X_train, y_train).predict(X_test)\n",
    "#         print(\"Number of mislabeled points out of a total of %d points: %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "        acc = accuracy_score(y_test,y_pred)\n",
    "        if acc < minAccuracyScore:\n",
    "            minAccuracyScore = acc\n",
    "            maxEffect = feature\n",
    "#         print(\"accuracy score without feature \" + feature + \" is \" + str(acc))\n",
    "\n",
    "        X = original\n",
    "    maxEffects.append(maxEffect)\n",
    "print(maxEffect, minAccuracyScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CT5e2CorticalRUnk', 'CT5e2CorticalRUnk', 'CT5e2CorticalRUnk', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular', 'CTIntraventricular']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'describe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f9af716242d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxEffects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmaxEffects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# we need to clean up the dataset - take away all datapoints that contain even a single 0 in the features that we are looking at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'describe'"
     ]
    }
   ],
   "source": [
    "print(maxEffects)\n",
    "\n",
    "for element in dict1:\n",
    "    dict1.\n",
    "# we need to clean up the dataset - take away all datapoints that contain even a single 0 in the features that we are looking at\n",
    "X = X.dropna(axis=0)\n",
    "X = X.astype('int64')\n",
    "X.describe()\n",
    "# for variable in features:\n",
    "#     X[variable] = X[variable].astype('int64')\n",
    "# X.dtypes\n",
    "# for variable in features:\n",
    "#     final = final[final[variable] != 0]\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories for classification\n",
    "\n",
    "* 0-3: 0\n",
    "* 4-9: 1\n",
    "* 9-13: 2\n",
    "* 14-17: 3\n",
    "* 18-21: 4\n",
    "* 21-25: 5\n",
    "* 26-30: 6\n",
    "* everything beyond 30: 7 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    15894.000000\n",
       "mean        20.731911\n",
       "std         19.034938\n",
       "min          0.000000\n",
       "25%          9.000000\n",
       "50%         17.000000\n",
       "75%         26.000000\n",
       "max        999.000000\n",
       "Name: DAYStoREHABadm, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    15974.000000\n",
       "mean         1.232628\n",
       "std          0.422520\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          1.000000\n",
       "75%          1.000000\n",
       "max          2.000000\n",
       "Name: DAYStoREHABadm, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = needLater\n",
    "y = pd.cut(y, [-1, 27, 1000], labels=False) + 1\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12759, 37) (12759,)\n",
      "(3190, 37) (3190,)\n",
      "Number of mislabeled points out of a total of 3190 points: 748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7655172413793103"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# to test other models, uncomment the line\n",
    "days_model = LogisticRegression(solver='lbfgs', multi_class='multinomial', max_iter=10000)\n",
    "y_pred = days_model.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total of %d points: %d\" % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "accuracy_score(y_test,y_pred)\n",
    "# days_model = DecisionTreeRegressor(random_state=1)\n",
    "# days_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5272727272727272"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# something to think about - do we want to use a uniform prior or not when we're doing Naive Bayes?\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB().fit(X_train, y_train)\n",
    "y_pred = mnb.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23242836541475956"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predicted_time = days_model.predict(X)\n",
    "mean_absolute_error(y, predicted_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan of action:\n",
    "* One-hot encoding - converts categorical variables to binary variables\n",
    "* Still need to find a way to deal with ordinal variables\n",
    "* Missing data can be encoded as a 0 on all the binary variables that result\n",
    "* Then use argmax\n",
    "* tfjs seems really cool, maybe consider that?\n",
    "* treating unknown variables as any other category for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
